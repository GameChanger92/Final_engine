# Final_engine
my-novel-engine

## Quick Demo

Generate your first episode with the integrated pipeline:

```bash
# Generate for default project
python -m src.main run --episode 1
cat projects/default/outputs/episode_1.txt

# Generate for a specific project
python -m src.main run --episode 1 --project-id demo_novel
cat projects/demo_novel/outputs/episode_1.txt
```

### Platform Style Configuration

The engine supports different writing styles for different platforms. Configure the style by setting the `PLATFORM` environment variable:

```bash
# Use munpia style (default)
PLATFORM=munpia python -m src.main run --episode 1

# Use kakao style  
PLATFORM=kakao python -m src.main run --episode 1
```

Available platforms:
- **munpia**: ëŒ€ì‚¬ ì¤‘ì‹¬ ê²½ì¾Œí•œ ìŠ¤íƒ€ì¼ (dialogue-focused, upbeat style)
- **kakao**: ì •ì„œì  ê¹Šì´ê° ìˆëŠ” ìŠ¤íƒ€ì¼ (emotionally deep style)

You can also set this in your `.env` file:
```
PLATFORM=munpia
```

This will run the complete pipeline:
1. **Arc Outliner** â†’ Creates basic story arc structure
2. **Beat Planner** â†’ Generates 3 story beats 
3. **Scene Maker** â†’ Creates ~10 scenes from the beats
4. **Context Builder** â†’ Combines scenes into narrative context
5. **Draft Generator** â†’ Produces final episode draft

The output is saved to `projects/{project-id}/outputs/episode_1.txt` with placeholder content that will be replaced by actual LLM-generated text in future versions.

## Guard Chain Testing

Test the complete guard validation system with the pipeline smoke test:

```bash
# Test default project
python scripts/run_pipeline.py --episodes 1-20

# Test specific project
python scripts/run_pipeline.py --project-id demo_novel --episodes 1-3
```

This runs all guards in sequence for each episode:
- **LexiGuard** â†’ Checks lexical quality (TTR, 3-gram duplication)
- **EmotionGuard** â†’ Monitors emotional transitions
- **ScheduleGuard** â†’ Validates foreshadow resolution compliance  
- **ImmutableGuard** â†’ Ensures character consistency
- **DateGuard** â†’ Checks chronological progression

Expected output format:
```
âœ… LexiGuard PASS
âœ… EmotionGuard PASS
âœ… ScheduleGuard PASS
âœ… ImmutableGuard PASS
âœ… DateGuard PASS
```

## í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì˜ˆì‹œ

Test anchor-driven integration flow to ensure all 5 anchor events appear within their expected episodes:

```bash
# Test default project
python scripts/run_anchor_flow.py

# Test specific project  
python scripts/run_anchor_flow.py --project-id demo_novel
```

This validates that all anchor events from `projects/{project-id}/data/anchors.json` appear within their target episodes (anchor_ep Â± 1) across a 20-episode simulation:

- **ANCHOR_01**: ì£¼ì¸ê³µ ì²« ë“±ì¥ (Episode 1)
- **ANCHOR_02**: ì²« ë²ˆì§¸ ì‹œë ¨ (Episode 5) 
- **ANCHOR_03**: ì¤‘ìš”í•œ ë§Œë‚¨ (Episode 10)
- **ANCHOR_04**: ê²°ì •ì  ì„ íƒ (Episode 15)
- **ANCHOR_05**: ë§ˆì§€ë§‰ ëŒ€ê²° (Episode 20)

Expected output:
```
ğŸ¯ TEST RESULT: PASS
   All anchor events were found in their expected episodes!
ğŸ‰ SUCCESS: All anchor events validated!
```

Exit codes:
- **0**: All 5 anchors found in correct episodes
- **1**: One or more anchors missing + failure log

### Other Commands

```bash
# Show pipeline information
python -m src.main info

# Generate different episodes for different projects
python -m src.main run --episode 5 --project-id default
python -m src.main run --episode 10 --project-id demo_novel

# Test guard chain with pipeline smoke test
python scripts/run_pipeline.py --project-id default --episodes 1-20
python scripts/run_pipeline.py --project-id demo_novel --episodes 5      # Single episode
python scripts/run_pipeline.py --project-id default --episodes 10-15     # Episode range
```
