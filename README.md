# Final_engine
my-novel-engine

## Quick Demo

Generate your first episode with the integrated pipeline:

```bash
# Generate for default project
python -m src.main run --episode 1
cat projects/default/outputs/episode_1.txt

# Generate for a specific project
python -m src.main run --episode 1 --project-id demo_novel
cat projects/demo_novel/outputs/episode_1.txt
```

### Platform Style Configuration

The engine supports different writing styles for different platforms. Configure the style by setting the `PLATFORM` environment variable:

```bash
# Use munpia style (default)
PLATFORM=munpia python -m src.main run --episode 1

# Use kakao style  
PLATFORM=kakao python -m src.main run --episode 1
```

Available platforms:
- **munpia**: ëŒ€ì‚¬ ì¤‘ì‹¬ ê²½ì¾Œí•œ ìŠ¤íƒ€ì¼ (dialogue-focused, upbeat style)
- **kakao**: ì •ì„œì  ê¹Šì´ê° ìˆëŠ” ìŠ¤íƒ€ì¼ (emotionally deep style)

You can also set this in your `.env` file:
```
PLATFORM=munpia
```

## Advanced Settings

### FAST MODE (ì„ë² ë”© ìŠ¤í‚µ)

ëŒ€ëŸ‰ ì„ë² ë”© ê³¼ì •(ìˆ˜ì²œ ê°œ ì¥ë©´)ì„ ìƒëµí•˜ê³  **ë¹ ë¥´ê²Œ íŒŒì´í”„ë¼ì¸**ì„ ëŒë¦¬ê³  ì‹¶ì„ ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤.

```bash
# .env íŒŒì¼ ì˜ˆì‹œ
FAST_MODE=1
```

### Testing Environment Flags

The engine supports multiple environment flags for testing and development:

#### Flag Priority Order
When multiple flags are set, the following priority order applies:
1. **UNIT_TEST_MODE=1** - Allows mocks to work in unit tests, takes precedence over FAST_MODE
2. **FAST_MODE=1** - Returns dummy values for fast execution  
3. **Normal execution** - Makes real API calls

#### Usage Examples
```bash
# Unit testing (mocks work, FAST_MODE ignored)
UNIT_TEST_MODE=1 FAST_MODE=1 pytest tests/

# Fast execution (dummy values, no API calls)
FAST_MODE=1 python scripts/run_pipeline.py

# Normal execution (real API calls)
python scripts/run_pipeline.py
```

#### CI/CD Configuration
- **Unit tests**: `UNIT_TEST_MODE=1 FAST_MODE=0` 
- **Benchmark tests**: `UNIT_TEST_MODE=0 FAST_MODE=1`

### LLM Temperature Control

The engine allows fine-tuning of creativity levels for different pipeline stages using temperature parameters:

```bash
# Experimental draft with higher creativity
export TEMP_DRAFT=0.85
python scripts/run_pipeline.py --project-id easy-money --episodes 1-1
```

Available temperature settings:
- **TEMP_DRAFT**: Controls creativity for final draft generation (default: 0.7)
- **TEMP_BEAT**: Controls creativity for beat planning (default: 0.3)  
- **TEMP_SCENE**: Controls creativity for scene generation (default: 0.6)

You can set these in your `.env` file:
```env
TEMP_DRAFT=0.7
TEMP_BEAT=0.3
TEMP_SCENE=0.6
```

The log output will show the temperature being used:
```
âœï¸  Draft Generatorâ€¦ (temperature=0.85, max_output_tokens=60000)
```

This will run the complete pipeline:
1. **Arc Outliner** â†’ Creates basic story arc structure
2. **Beat Planner v2** â†’ Generates 3-Act structure with 6 sequences and 4 beats each
3. **Scene Maker v2** â†’ Creates 8-12 ScenePoints from the beats with enhanced metadata
4. **Context Builder** â†’ Combines scenes into narrative context
5. **Draft Generator** â†’ Produces final episode draft

### Scene Maker v2 Structure

Scene Maker v2 now generates 8-12 detailed ScenePoints per beat with enhanced metadata:

**ScenePoint Schema:**
```yaml
scene_03:
  pov: side          # main | side
  purpose: "í›ˆë ¨ ê²°ê³¼ ë°œí‘œ"
  tags: ["ì§€ì•„", "êµê´€", "í›ˆë ¨ì¥"]
  desc: "ì§€ì•„ëŠ” ë‚®ì€ ì ìˆ˜ì—ë„ ë¶ˆêµ¬í•˜ê³  êµê´€ì˜ ì‹œì„ ì„ ëŠë‚€ë‹¤â€¦"
```

**Key Features:**
- **pov**: Perspective type (`main` for main character, `side` for supporting character)
- **purpose**: Brief narrative purpose of the scene (Korean, 10-30 characters)
- **tags**: List of characters and locations mentioned in the scene
- **desc**: Detailed scene description (Korean, 50-150 characters)

The Scene Maker v2 also stores scene metadata in the vector database for enhanced search and retrieval capabilities.

### Beat Planner v2 Structure

The Beat Planner now generates beats in a structured 3-Act format:

**3-Act Structure:**
- **Act 1 (Setup)**: Sequences 1-2 
- **Act 2 (Confrontation)**: Sequences 3-4
- **Act 3 (Resolution)**: Sequences 5-6

**Each sequence contains 4 beats:**
- `beat_1`, `beat_2`, `beat_3`: Story progression beats
- `beat_tp`: Turning point for narrative tension

**Output Example:**
```yaml
ep_42:
  seq_2:
    beat_1: "í›ˆë ¨ì—ì„œ êµ´ìš•ì ì¸ íŒ¨ë°°"
    beat_2: "ë¹„ë°€ ì—°ìŠµ ì‹œì‘"
    beat_3: "ì²« ê°€ì‹œì  ì„±ì¥"
    beat_tp: "êµê´€ì˜ ì˜ì‹¬ ì–´ë¦° ì‹œì„ "
```

This structure is fully compatible with Scene Maker v2 and uses the `seq_x/beat_y` format for seamless integration.

### Guard Configuration

The engine supports advanced quality control through configurable guards. Set thresholds in your `.env` file:

```bash
# Self-Critique Guard: LLM-based fun & logic evaluation
MIN_CRITIQUE_SCORE=7.0    # Minimum score (1-10) for fun and logic

# Other guard settings (examples)
# LEXI_TTR_THRESHOLD=0.17
# EMOTION_DELTA_MAX=0.7
```

**Self-Critique Guard Features:**
- Uses Gemini 2.5 Pro to evaluate generated content
- Scores content on two dimensions: **ì¬ë¯¸ (fun)** and **ê°œì—°ì„± (logic)**
- Raises RetryException if `min(fun, logic) < MIN_CRITIQUE_SCORE`
- Integrated into Beat Planner, Scene Maker, and Draft Generator workflows
- Automatic retry with exponential backoff on low scores

Test with different thresholds:
```bash
# Strict quality control
MIN_CRITIQUE_SCORE=8.5 python scripts/run_pipeline.py --episode 2

# Relaxed quality control  
MIN_CRITIQUE_SCORE=6.0 python scripts/run_pipeline.py --episode 2
```

The output is saved to `projects/{project-id}/outputs/episode_1.txt` with placeholder content that will be replaced by actual LLM-generated text in future versions.

## Guard Chain Testing

Test the complete guard validation system with the pipeline smoke test:

```bash
# Test default project
python scripts/run_pipeline.py --episodes 1-20

# Test specific project
python scripts/run_pipeline.py --project-id demo_novel --episodes 1-3
```

This runs all guards in sequence for each episode:
- **LexiGuard** â†’ Checks lexical quality (TTR, 3-gram duplication)
- **EmotionGuard** â†’ Monitors emotional transitions
- **ScheduleGuard** â†’ Validates foreshadow resolution compliance  
- **ImmutableGuard** â†’ Ensures character consistency
- **DateGuard** â†’ Checks chronological progression
- **AnchorGuard** â†’ Validates anchor event compliance
- **RuleGuard** â†’ Checks forbidden pattern violations
- **RelationGuard** â†’ Monitors character relationship changes
- **PacingGuard** â†’ Validates narrative pacing balance
- **CritiqueGuard** â†’ LLM-based fun & logic evaluation

Expected output format:
```
âœ… LexiGuard PASS
âœ… EmotionGuard PASS
âœ… ScheduleGuard PASS
âœ… ImmutableGuard PASS
âœ… DateGuard PASS
âœ… AnchorGuard PASS
âœ… RuleGuard PASS
âœ… RelationGuard PASS
âœ… PacingGuard PASS
âœ… CritiqueGuard PASS
```

## Batch Execution

Run large episode batches (1-240) with the Full-Season Runner for comprehensive story generation and KPI tracking:

```bash
# Generate episodes 1-10 with default project
python scripts/run_full_season.py --episodes 1-10

# Generate full season (1-240) with specific project
python scripts/run_full_season.py --project-id demo_novel --episodes 1-240

# Generate custom range with style override
python scripts/run_full_season.py --episodes 1-50 --style munpia

# Generate specific episodes
python scripts/run_full_season.py --episodes 1,10,20
```

### Features

- **Automated Episode Generation**: Runs the complete pipeline for each episode in the range
- **KPI Tracking**: Collects quality metrics across all episodes:
  - `avg_fun`: Average entertainment score from Self-Critique (1-10)
  - `avg_logic`: Average logic/coherence score from Self-Critique (1-10)  
  - `guard_pass_rate`: Percentage of episodes passing all 10 guards
  - `avg_chars`: Average character count per episode
- **Progress Monitoring**: Real-time console output showing episode-by-episode results
- **HTML Reporting**: Automatic generation of `reports/season_KPI.html` with charts and tables

### Expected Output

```
ğŸš€ Starting Full Season Runner: Episodes 1-10 (Project: default)
============================================================

ğŸ“º Processing Episode 1...
   âœ… PASS | Fun: 8.1 | Logic: 8.4 | Chars: 5,650
----------------------------------------
ğŸ“º Processing Episode 2...
   âœ… PASS | Fun: 8.3 | Logic: 8.2 | Chars: 5,420
...

============================================================
ğŸ“Š KPI SUMMARY
============================================================
ğŸ“ˆ Fun Score (avg):      8.1/10
ğŸ§  Logic Score (avg):    8.4/10
ğŸ›¡ï¸  Guard Pass Rate:      92%
ğŸ“ Average Characters:   5,650
â±ï¸  Total Runtime:        45.2s
âœ… Passed Episodes:      9
âŒ Failed Episodes:      1

ğŸ“Š KPI Summary â€” fun 8.1 / logic 8.4 / guard_pass 92 % / avg chars 5,650
âœ”ï¸  Report saved to reports/season_KPI.html
```

The HTML report includes:
- Interactive KPI dashboard with charts
- Episode-by-episode detailed results table
- Visual score distribution graphs
- Overall statistics summary

## í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì˜ˆì‹œ

Test anchor-driven integration flow to ensure all 5 anchor events appear within their expected episodes:

```bash
# Test default project
python scripts/run_anchor_flow.py

# Test specific project  
python scripts/run_anchor_flow.py --project-id demo_novel
```

This validates that all anchor events from `projects/{project-id}/data/anchors.json` appear within their target episodes (anchor_ep Â± 1) across a 20-episode simulation:

- **ANCHOR_01**: ì£¼ì¸ê³µ ì²« ë“±ì¥ (Episode 1)
- **ANCHOR_02**: ì²« ë²ˆì§¸ ì‹œë ¨ (Episode 5) 
- **ANCHOR_03**: ì¤‘ìš”í•œ ë§Œë‚¨ (Episode 10)
- **ANCHOR_04**: ê²°ì •ì  ì„ íƒ (Episode 15)
- **ANCHOR_05**: ë§ˆì§€ë§‰ ëŒ€ê²° (Episode 20)

Expected output:
```
ğŸ¯ TEST RESULT: PASS
   All anchor events were found in their expected episodes!
ğŸ‰ SUCCESS: All anchor events validated!
```

Exit codes:
- **0**: All 5 anchors found in correct episodes
- **1**: One or more anchors missing + failure log

### Other Commands

```bash
# Show pipeline information
python -m src.main info

# Generate different episodes for different projects
python -m src.main run --episode 5 --project-id default
python -m src.main run --episode 10 --project-id demo_novel

# Test guard chain with pipeline smoke test
python scripts/run_pipeline.py --project-id default --episodes 1-20
python scripts/run_pipeline.py --project-id demo_novel --episodes 5      # Single episode
python scripts/run_pipeline.py --project-id default --episodes 10-15     # Episode range
```

## Local Style Checking

### Pre-commit Hooks (Recommended)

Install pre-commit hooks to automatically format code on each commit:

```bash
# Install pre-commit (one-time setup)
pip install pre-commit

# Install the git hook scripts
pre-commit install

# Optionally, run against all files
pre-commit run --all-files
```

### Manual Style Checking

Before submitting a PR, check your code style locally:

```bash
# ì „ì²´ ê²€ì‚¬
ruff check .
black --check --diff .

# ìë™ ìˆ˜ì •
ruff check . --fix
black .
```
